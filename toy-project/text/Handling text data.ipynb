{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트와 시퀀스를 위한 딥러닝\n",
    "- 텍스트, 시계열 또는 일반적인 시퀀스 데이터를 처리할 수 있는 딥러닝 모델을 살펴본다.\n",
    "- 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 순환신경망과 1D convnet 두 가지.\n",
    "- 1D convnet은 2D convnet의 1차원 버전.\n",
    "- 다음 애플리케이션들이 이런 알고리즘을 사용한다.\n",
    "    - 문서 분류나 시계열 분류, 예를 들어 글의 주제나 책의 저자 식별하기.\n",
    "    - 시계열 비교, 예를 들어 두 문서나 두 주식 사격이 얼마나 밀접하게 관련이 있는지 추정하기.\n",
    "    - 시퀀스-투-시퀀스 학습, 예를 들어 영어 문장을 프랑스어로 변환하기.\n",
    "    - 감성 분석, 예를 들어 트윗이나 영화 리뷰가 긍정적인지 부정적인지 분류하기.\n",
    "    - 시계열 예측, 예를 들으어 어떤 지역의 최근 날씨 데이터가 주어졌을 때 향후 날씨 예측하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 데이터 다루기\n",
    "- 텍스트는 단어의 시퀀스나 문자의 시퀀스로 이해할 수 있다. 보통 단어 수준으로 작업하는 경우가 많다.\n",
    "- 사람처럼 진짜 텍스트를 이해하는 것은 아니다. 문자 언어에 대한 통계적 구조를 만들어 간단한 텍스트 문제를 해결한다.\n",
    "- 단어, 문장, 문다에 적용한 패턴인식.\n",
    "\n",
    "### 텍스트 벡터화\n",
    "텍스트 원본을 입력으로 사용하지 못하기 때문에 텍스트를 수치형 텐서로 변환하는 과정을 거친다. 이를 **텍스트 벡터화**라고 한다.\n",
    "\n",
    "- 텍스트를 단어로 나우고 각 단어를 하나의 벡터로 변환.\n",
    "- 텍스트를 문자로 나우고 각 문자를 하나의 벡터로 변환.\n",
    "- 텍스트에서 단어나 문자의 n-gram을 추출하여 각 n-gram을 하나의 벡터로 변환한다. n-gram은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출한다.\n",
    "\n",
    "텍스트를 나누는 단위를 **토큰**이라고 하고 텍스트를 토큰으로 나누는 작업을 **토큰화**라고 한다. \n",
    "\n",
    "모든 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 **토큰**에 **수치형 벡터를 연결**하는 것으로 이루어진다.\n",
    "\n",
    "이런 **벡터**는 **시퀀스 텐서**로 묶여져서 심층 신경망에 주입된다.\n",
    "\n",
    "### 토큰과 벡터를 연결하는 방법\n",
    "- 원-핫 인코딩\n",
    "- 토큰 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어와 문자의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            # 단어마다 고유한 인덱스를 할당한다. 인덱스 0은 사용하지 않는다.\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            \n",
    "max_length = 10 # 샘플을 벡터로 변환한다. 각 샘플에서 max_length까지 단어만 사용한다.\n",
    "\n",
    "# 결과를 저장할 배열\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                         max_length,\n",
    "                         max(token_index.values()) + 1))\n",
    "                   \n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
